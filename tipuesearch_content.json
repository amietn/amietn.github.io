{"pages":[{"title":"About","text":"Who am I? Welcome to my blog. This is a space where I share knowledge worth spreading. It's also a way for me to keep track of projects I worked on and conference presentations I gave. My name is Nils, and I'm a security researcher working for a global security company. I've been doing applied research for nearly a decade and presented my work at various security conferences throughout that time. My work includes building new security tooling, implementing new attacks, and assessing security at scale. Before that, I was a software developer, mostly writing Java code. I bridge the gap between the software development and security industries. I'm a big fan of Linux and I enjoy keeping up with the latest technology and the associated security aspects. I'm a gourmet and really enjoy good food. I like to cook on my own, gaming and watching movies when I'm not spending quality time with my dear ones. This website This blog is powered by Pelican . This is a custom theme based on Tabler . Github: amietn","tags":"pages","url":"https://tome.one/pages/about.html"},{"title":"Conferences","text":"Here are the conferences I attended. 2023 Ph0wn CTF, Sophia Antipolis, France Black Alps, Yverdon-les-Bains, Switzerland BlackHat USA 2023, Las Vegas, USA DEF CON 31, Las Vegas, USA 2022 Black Alps 2022, Yverdon-les-Bains, Switzerland MCH 2022, Zeewolde, Netherlands SSTIC 2022, Rennes, France Nullcon Berlin 2022, Berlin, Germany 2021 Black Alps BBQ 2021, Cheyres, Switzerland Pass-the-SALT 2021, virtual 2020 Pass-the-SALT 2020, virtual 2019 Black Alps 19, Yverdon-les-Bains, Switzerland DEF CON 27, Las Vegas, USA Black Hat USA 2019, Las Vegas, USA 2018 Black Alps 2018, Yverdon-les-Bains, Switzerland DEF CON 26, Las Vegas, USA Black Hat USA 2018, Las Vegas, USA 2017 Spark Summit Europe 2017, Dublin, Ireland 2016 Spark Summit Europe 2016, Brussels, Belgium","tags":"pages","url":"https://tome.one/pages/conferences.html"},{"title":"Links","text":"svenamiet.ch deadc0de.re william-droz.com kobl.one","tags":"pages","url":"https://tome.one/pages/links.html"},{"title":"Projects","text":"Here are some of the projects I built. fuzzomatic python Many developers do not do fuzzing because it may seem difficult or is sometimes forgotten. However, it is a very effective testing technique that finds bugs at runtime. Fuzzomatic lowers the barrier to entry and can automatically generate libFuzzer fuzz targets for projects written in Rust, completely from scratch. It uses OpenAI's API to aid in generating fuzz targets that successfully compile. Fuzzomatic found 14 bugs in the top 50 most starred parser library Rust projects on Github for just under $3 of OpenAI API costs. Read the blog post linked in the Github repository's README for more details. Github: https://github.com/kudelskisecurity/fuzzomatic oramfs rust Data encryption prevents others from seeing what the data is. However, when the encrypted data is stored remotely on a cloud provider, such as Google Drive or Dropbox, the cloud provider can still see how the data is accessed. For example, whether it is read or written to, and which parts of the data are accessed. In some cases, this can leak some information. ORAMs \"scramble\" or hide these read and write access patterns. OramFS is an implementation of an ORAM file system written in Rust using FUSE. The ORAM scheme that was implemented is PathORAM. It is storage agnostic and will work with any remote host provider such as Google Drive, Dropbox, etc. that can be mounted as a local file system. Thanks to OramFS, not only the data is encrypted, but also the access patterns are masked to the cloud provider. This added level of privacy comes at a performance cost. Github: https://github.com/kudelskisecurity/oramfs vcsi python Generate video previews as an image. The generated image is a collage of multiple screenshots taken at various points during the video. These collages are called video contact sheets and are useful to quickly get an idea of what a video looks like. Github: https://github.com/amietn/vcsi For more open source projects, see my Github profile .","tags":"pages","url":"https://tome.one/pages/projects.html"},{"title":"Publications","text":"Articles Blockchain Vulnerabilities in Practice (2021) This is an article I wrote for Digital Threats: Research and Practice (DTRAP) Blog posts I wrote multiple blog posts for the Kudelski Security research blog","tags":"pages","url":"https://tome.one/pages/publications.html"},{"title":"Talks","text":"Here are the talks I gave. 2023 Security Keys workshop Ph0wn CTF, Sophia Antipolis, France with my colleague Sylvain Pelissier Slides Polynonce: An ECDSA Attack and Polynomial Dance DEF CON 31, Las Vegas, USA with my colleague Marco Macchetti Video Slides 2022 The smart home I didn't ask for Black Alps 2022, Yverdon-les-Bains, Switzerland Video Slides The smart home I didn't ask for MCH 2022, Zeewolde, Netherlands Video Video2 Slides Analyse forensique de la m√©moire de GnuPG (in French) SSTIC 2022, Rennes, France with my colleague Sylvain Pelissier Video Slides Paper GPG memory forensics Nullcon Berlin 2022, Berlin, Germany with my colleague Sylvain Pelissier Video Slides 2021 ORAMFS: Achieving Storage-Agnostic Privacy Pass the SALT 2021, virtual with my colleague Tommaso Gagliardoni Video Slides 2020 Replacing passwords with FIDO2 Pass the SALT 2020, virtual Video Slides 2019 Blockchain vulnerabilities and exploitation in practice Black Alps 19, Yverdon-les-Bains, Switzerland 3-hour workshop on Ethereum smart contracts and core blockchain with FumbleChain Video Slides FumbleChain: A Purposefully Vulnerable Blockchain Blockchain Village @ DEFCON 27, Las Vegas, USA Slides FumbleChain: A Purposefully Vulnerable Blockchain Black Hat USA 2019 Arsenal FumbleChain demo at Arsenal booth Slides 2018 Reaping and breaking keys at scale: when crypto meets big data DEF CON 26, Las Vegas, USA with my colleague Yolan Romailler Video Slides","tags":"pages","url":"https://tome.one/pages/talks.html"},{"title":"Passkeys and Security Key Shopping Guide","text":"User authentication has long been relying on passwords. However, passwords come with a ton of problems. They're hard to remember, they can be reused across websites and services and are sensitive to phishing and data leaks. Also, password policies are a major pain point for users. Password managers solve the \"hard to remember\" piece, but are just a workaround. They're not solving the actual problem. We need to get rid of passwords. Passkeys You probably have heard of Passkeys . They're a new way to sign in to websites and applications without passwords. Passkeys can be stored on your laptop, on your smartphone or on a hardware security key, such as a Yubikey. For the average user, storing one's passkeys in their smartphone and having them synced in Google's or Apple's cloud is probably the most user-friendly solution and is what I expect most people will do in the near future. However, this solution comes at a cost. You give up some security because you trust Google, Apple or whatever third-party passkey provider you choose to store your passkeys. This is where hardware-based security keys shine, because the passkeys you store into those can never leave the security key. They remain offline and you keep control or your passkeys. Security keys Shopping for security keys these days is hard. Vendors like Yubico, Google, Nitrokey and SoloKeys to name a few, make it especially hard for you to know what you're getting. Indeed, these security keys come with a wide array of features, like FIDO2, PGP, PIV, OTP, and more. For passkeys, we only care about FIDO2 support because Passkeys are just FIDO2 credentials, and this is the topic of this blog post so we'll only cover that today. Indeed, 99% of security key vendors fail to provide detailed technical specifications of what their FIDO2 security key supports, so you never know what you're getting and may be disappointed upon delivery. For example, in November last year, Google announced the latest iteration of their Titan Security Key and claimed it can store up to 250 unique passkeys. This sounded super nice because current models on the market previously only allowed to store up to 25, 50 or maybe 100 passkeys. Before that, no product ever allowed to store that many passkeys. I decided to buy that new Titan Security Key because of that claim, and it arrived the next day. When it arrived, I immediately plugged it in and ran ctapcli info to see what FIDO2 features and options it supported: $ ctapcli info Get the Authenticator infomation. Get all data. - versions = [ \"FIDO_2_0\" , \"U2F_V2\" ] - extensions = [ \"credProtect\" , \"hmac-secret\" ] - aaguid ( 16 ) = 42B4FB4A286643B29BF76C6669C2E5D3 - options = [( \"rk\" , true ) , ( \"clientPin\" , true )] - max_msg_size = 2200 - pin_uv_auth_protocols = [ 1 ] - max_credential_count_in_list = 0 - max_credential_id_length = 0 - transports = [] - algorithms = [] - max_serialized_large_blob_array = 0 - force_pin_change = false - min_pin_length = 0 - firmware_version = 0 - max_cred_blob_length = 0 - max_rpids_for_set_min_pin_length = 0 - preferred_platform_uv_attempts = 0 - uv_modality = 0 - remaining_discoverable_credentials = 0 Error: Invalid item And here's the output of fido2-token : $ fido2-token -L /dev/hidraw0: vendor = 0x18d1, product = 0x9470 ( Google Titan Security Key v2 ) $ fido2-token -I /dev/hidraw0 proto: 0x02 major: 0x02 minor: 0x00 build: 0x03 caps: 0x05 ( wink, cbor, msg ) version strings: FIDO_2_0, U2F_V2 extension strings: credProtect, hmac-secret aaguid: 42b4fb4a286643b29bf76c6669c2e5d3 options: rk, clientPin fwversion: 0x0 maxmsgsiz: 2200 maxcredcntlst: 0 maxcredlen: 0 maxlargeblob: 0 pin protocols: 1 pin retries: 8 pin change required: false uv retries: undefined As you can see, the new Titan Security Key (2023 model) doesn't support credential management. Indeed, it may be able to store up to 250 resident credentials, but there's no way to selectively delete or enumerate those credentials. This is a big letdown for me because when I do fill those 250 slots, the only way for me to make some free space is to completely reset the key and lose all my passkeys, which is definitely not what I'd like. I also recently purchased a Security Key C NFC by Yubico. This model can only store up to 25 passkeys, but it does support credential management: $ ctapcli info ... - options = [( \"rk\" , true ) , ( \"up\" , true ) , ( \"plat\" , false ) , ( \"clientPin\" , true ) , ( \"credentialMgmtPreview\" , true )] Indeed, it supports enumerating resident credentials and tells how many free slots are left: $ ctapcli cred PIN: Enumerate discoverable credentials. - existing discoverable credentials: 1 /24 - rp: ( id: github.com, name: ) - credential: ( id: ***, name: ***, display_name: *** ) We can see that I have one passkey stored and 24 free slots left, so the Security Key C NFC really does support storing up to 25 passkeys. What's nice about this model, is that it also reports that it supports both ECDSA with the P-256 curve, and EdDSA for signatures . I still do wish that Yubico makes a newer model with the same features but with more storage for passkeys. Features to look for when choosing a security key I was really disappointed when I found out that the security key I bought didn't support what I thought was a very important feature. There was no way to tell whether it supported it or not. The official product store page didn't list any detailed technical specifications. To help you avoid ending up in the same uncomfortable situation I found myself in, here's a quick guide to buying a new security key. When I'm shopping for a new security key, this is what I look for. 1) FIDO2 support The security key must support FIDO2 to be able to store passkeys. 2) How you're going to connect the security key Does it support USB type A or USB type C? Does it support NFC? 3) Max number of resident credentials Is that 25? 50? 250? Think of how many passkeys you'll need to store. Is this going to be enough? This information may not even be shown on the vendor's official site. Sometimes, the best way to make sure, is to ask someone who already owns that security key model to check that for you. You may as well find the appendix of this blog post useful, because it contains the output of authenticatorGetInfo for multiple FIDO2 security key models I own. Maybe the one you're interested in is in that list. 4) hmac-secret extension support This CTAP extension is required by some applications, such as LUKS. Look for the hmac-secret value in the extensions field of the CTAP authenticatorGetInfo command output. 5) credProtect extension support The credProtect extension is used to enforce per-credential policies. During creation of a new credential (or passkey), the credProtect extension can be set to, for example, userVerificationRequired . If the authenticator supports the credProtect extension, then this passkey can only ever be used with user verification (PIN, fingerprint, etc.). This gives you the peace of mind that even if your security key is stolen, your passkeys cannot be used without your PIN or fingerprint. 6) CTAP 2.1 support The security key implements CTAP 2.1 or higher. This is usually signaled by the presence of either the FIDO_2_1_PRE or the FIDO_2_1 value in the versions field of the CTAP authenticatorGetInfo command output. There are security vulnerabilities that can be exploited in CTAP 2.0, such as PIN bypasses, especially with respect to the use of the hmac-secret extension. You definitely want this if you're going to use a security key to unlock LUKS partitions. 7) Credential management support Look for the presence of the credMgmt or credentialMgmtPreview value in the options field of the CTAP authenticatorGetInfo command output. Without credential management support, there's no way to delete individual passkeys and free up space. When the security key gets full, the only option is to reset it, and therefore lose all passkeys stored on it. Credential management also allows for listing stored credentials and seeing how many free slots are left. Storing resident credentials is the only option for username-less sign in. If the credentials are not stored on the authenticator, the website you're signing in to has no way to know your username, and you'll have to manually type it. When storing resident credentials, those credentials become discoverable, and can be enumerated. Therefore, the username can be autofilled and your web browser will show you a nice UI where you can select which account you'd like to use in case you have used Passkeys for multiple accounts on a same website. For example, if you have more than one Google account. 8) Does it support biometrics? Do you need to perform user verification with your fingerprint? Usually, PIN is supported, but some security keys have a fingerprint reader and can store your fingerprint. One advantage of using biometrics is that user presence (where you tap the security key) and user verification (where the user's identity is verified) can be performed in a single gesture: by placing your finger on the fingerprint reader. Whereas with a PIN, user presence must still be done by touching the security key and the PIN has to be entered in another step. If a fingerprint reader sounds interesting, you may want to verify how many fingerprints can be stored on that security key. It may not be possible to enroll and store fingerprints for all your fingers. Some models only allow to store a maximum of 3 fingerprints, for example, and as usual, this number is never specified. You discover it after having purchased the security key. To my knowledge, there is no command that will directly tell how many fingerprints can be stored. However, when enrolling a new fingerprint, the command will fail when there are no more free slots. Note that usually, if there's a fingerprint reader, there's no NFC support, so you'll have to make a choice here. 9) Do you care about hardware attacks? Is a physical attack in your threat model? Do you need to be covered in case your security key is stolen by an attacker? In that case, you need to make sure your security key uses a secure element to store credentials, and that there are no known vulnerabilities on that secure element. Also, make sure that the secure element is actually used. Some vendors sell security keys that contain a secure element but which is not used at all (see the IMPORTANT NOTE on that page). Summary and other criteria To summarize, mainly look for the following things: Does it support FIDO2? USB Type-A or Type-C? NFC support? Max resident credentials hmac-secret extension support credProtect extension support CTAP 2.1 support Credential management support Biometrics Secure element But there are a few more things also worth looking for: Price Form factor / design (sticks out of laptop or fits flush, slim/stylish or bulky) Availability and quality of documentation Product certifications, such as FIPS 140-2, NIST SP800-63B or CC EAL (you may need this for enterprise scenarios) Other supported applications (OTP, PGP, static password, PIV) Open source firmware Upgradable firmware Does it work inside virtual machines? (this is a pretty niche use-case, but we've seen models work better than others inside VMs with USB passthrough) So which key is the best on the market today? I'm not affiliated with Yubico, but I do believe they make pretty good, secure and affordable products, that come with good documentation. Especially, after having used security keys made by other vendors. They may not list all the technical details of their products on their website (see the appendix for more info), but I can easily recommend the $25 Security Key C NFC if Passkeys are all you care about and it's for personal use. I would also recommend buying a pair of these and register both security keys as a backup on all the websites you use, in case you lose one so that you don't get locked out. The only downside I see is the number of resident credentials is limited to 25. Hopefully they release a new model that bumps that number sooner than later. If you need more applications like PGP, OTP or PIV, I would go for the Yubikey 5, which is essentially the same thing, but with more applications and also for double the price. But these days, I think Passkeys/FIDO2 is the killer app. Do you really need support for those other applications? Conclusions For the average person, maybe a security key is overkill and storing passkeys in their smartphone, plus cloud-sync is enough. But if you're concerned about third-parties storing your passkeys, this blog post gave an overview of features to look for and pitfalls to avoid. Choosing a security key depends on your use case, and I gave a list of these features. Think of which ones make sense to you and the choice will become easier. Appendix Here are some technical details about the security keys I own. For each security key, I will include as much information as possible, including the output of the CTAP authenticatorGetInfo command. Click on a security below to show more details. Security Key C NFC by Yubico (2023) This one was purchased in late 2023 for $29 off the official Yubico website. It's the newer model which is now black. It previously was blue. It's running firmware version 5.4.3. $ ctapcli info Get the Authenticator infomation. Get all data. - versions = [ \"U2F_V2\" , \"FIDO_2_0\" , \"FIDO_2_1_PRE\" ] - extensions = [ \"credProtect\" , \"hmac-secret\" ] - aaguid ( 16 ) = A4E9FC6D4CBE4758B8BA37598BB5BBAA - options = [( \"rk\" , true ) , ( \"up\" , true ) , ( \"plat\" , false ) , ( \"clientPin\" , true ) , ( \"credentialMgmtPreview\" , true )] - max_msg_size = 1200 - pin_uv_auth_protocols = [ 2 , 1 ] - max_credential_count_in_list = 8 - max_credential_id_length = 128 - transports = [ \"nfc\" , \"usb\" ] - algorithms = [( \"alg\" , \"-7\" ) , ( \"type\" , \"public-key\" ) , ( \"alg\" , \"-8\" ) , ( \"type\" , \"public-key\" )] - max_serialized_large_blob_array = 0 - force_pin_change = false - min_pin_length = 4 - firmware_version = 328707 - max_cred_blob_length = 0 - max_rpids_for_set_min_pin_length = 0 - preferred_platform_uv_attempts = 0 - uv_modality = 0 - remaining_discoverable_credentials = 0 Error: Invalid item $ fido2-token -I /dev/hidraw0 proto: 0x02 major: 0x05 minor: 0x04 build: 0x03 caps: 0x05 ( wink, cbor, msg ) version strings: U2F_V2, FIDO_2_0, FIDO_2_1_PRE extension strings: credProtect, hmac-secret transport strings: nfc, usb algorithms: es256 ( public-key ) , eddsa ( public-key ) aaguid: a4e9fc6d4cbe4758b8ba37598bb5bbaa options: rk, up, noplat, clientPin, credentialMgmtPreview fwversion: 0x50403 maxmsgsiz: 1200 maxcredcntlst: 8 maxcredlen: 128 maxlargeblob: 0 minpinlen: 4 pin protocols: 2 , 1 pin retries: 8 pin change required: false uv retries: undefined $ ctapcli cred PIN: Enumerate discoverable credentials. - existing discoverable credentials: 1 /24 - rp: ( id: github.com, name: ) - credential: ( id: ***, name: ***, display_name: *** ) Security Key NFC by Yubico (2019) This is the blue security key. I bought this one from the official Yubico store in February 2019 for $20, plus shipping fee, for a total of $25. It came with firmware version 5.1.2. It doesn't support credential management. Note that Yubico's firmware is proprietary and firmware upgrades are NOT possible. The only way to get a newer firmware is to purchase a new security key. $ ykman info Device type: Security Key NFC Firmware version: 5 .1.2 Form factor: Keychain ( USB-A ) Enabled USB interfaces: FIDO NFC transport is enabled. Applications USB NFC OTP Not available Not available FIDO U2F Enabled Enabled FIDO2 Enabled Enabled OATH Not available Not available PIV Not available Not available OpenPGP Not available Not available YubiHSM Auth Not available Not available $ ctapcli info Get the Authenticator infomation. Get all data. - versions = [ \"U2F_V2\" , \"FIDO_2_0\" ] - extensions = [ \"hmac-secret\" ] - aaguid ( 16 ) = 6D44BA9BF6EC2E49B9300C8FE920CB73 - options = [( \"rk\" , true ) , ( \"up\" , true ) , ( \"plat\" , false ) , ( \"clientPin\" , true )] - max_msg_size = 1200 - pin_uv_auth_protocols = [ 1 ] - max_credential_count_in_list = 0 - max_credential_id_length = 0 - transports = [] - algorithms = [] - max_serialized_large_blob_array = 0 - force_pin_change = false - min_pin_length = 0 - firmware_version = 0 - max_cred_blob_length = 0 - max_rpids_for_set_min_pin_length = 0 - preferred_platform_uv_attempts = 0 - uv_modality = 0 - remaining_discoverable_credentials = 0 Error: Invalid item $ fido2-token -I /dev/hidraw0 proto: 0x02 major: 0x05 minor: 0x01 build: 0x02 caps: 0x05 ( wink, cbor, msg ) version strings: U2F_V2, FIDO_2_0 extension strings: hmac-secret aaguid: 6d44ba9bf6ec2e49b9300c8fe920cb73 options: rk, up, noplat, clientPin fwversion: 0x0 maxmsgsiz: 1200 maxcredcntlst: 0 maxcredlen: 0 maxlargeblob: 0 pin protocols: 1 pin retries: 8 pin change required: false uv retries: undefined Goldengate G320 by eWBM Note that the G320 is the USB type-C model and the G310 is the same but with a USB type-A connector. I was contacted by eWBM and given these two models (G310 and G320) for free in October 2019 for an evaluation of their products. The maximum number of fingerprints that can be stored is 3. $ ctapcli info Get the Authenticator infomation. Get all data. - versions = [ \"U2F_V2\" , \"FIDO_2_0\" , \"FIDO_2_1_PRE\" ] - extensions = [ \"credProtect\" , \"hmac-secret\" ] - aaguid ( 16 ) = 87DBC5A14C944DC88A4797D800FD1F3C - options = [( \"rk\" , true ) , ( \"up\" , true ) , ( \"uv\" , true ) , ( \"plat\" , false ) , ( \"clientPin\" , true ) , ( \"credentialMgmtPreview\" , true ) , ( \"userVerificationMgmtPreview\" , true )] - max_msg_size = 2048 - pin_uv_auth_protocols = [ 1 ] - max_credential_count_in_list = 6 - max_credential_id_length = 192 - transports = [ \"usb\" ] - algorithms = [] - max_serialized_large_blob_array = 0 - force_pin_change = false - min_pin_length = 0 - firmware_version = 0 - max_cred_blob_length = 0 - max_rpids_for_set_min_pin_length = 0 - preferred_platform_uv_attempts = 0 - uv_modality = 0 - remaining_discoverable_credentials = 0 Error: Invalid item $ fido2-token -I /dev/hidraw0 proto: 0x02 major: 0x02 minor: 0x09 build: 0x65 caps: 0x0d ( wink, cbor, nomsg ) version strings: U2F_V2, FIDO_2_0, FIDO_2_1_PRE extension strings: credProtect, hmac-secret transport strings: usb aaguid: 87dbc5a14c944dc88a4797d800fd1f3c options: rk, up, uv, noplat, clientPin, credentialMgmtPreview, userVerificationMgmtPreview fwversion: 0x0 maxmsgsiz: 2048 maxcredcntlst: 6 maxcredlen: 192 maxlargeblob: 0 pin protocols: 1 pin retries: 8 pin change required: false uv retries: undefined sensor type: 1 ( touch ) max samples: 5 $ ctapcli cred PIN: Enumerate discoverable credentials. - existing discoverable credentials: 0 /100 No discoverable credentials. Titan Security Key v2 by Google (2023) This is the USB type-C model, also known as model K52T. I purchased this model from the official Google Store in November 2023 for $35. It doesn't support credential management! $ ctapcli info Get the Authenticator infomation. Get all data. - versions = [ \"FIDO_2_0\" , \"U2F_V2\" ] - extensions = [ \"credProtect\" , \"hmac-secret\" ] - aaguid ( 16 ) = 42B4FB4A286643B29BF76C6669C2E5D3 - options = [( \"rk\" , true ) , ( \"clientPin\" , true )] - max_msg_size = 2200 - pin_uv_auth_protocols = [ 1 ] - max_credential_count_in_list = 0 - max_credential_id_length = 0 - transports = [] - algorithms = [] - max_serialized_large_blob_array = 0 - force_pin_change = false - min_pin_length = 0 - firmware_version = 0 - max_cred_blob_length = 0 - max_rpids_for_set_min_pin_length = 0 - preferred_platform_uv_attempts = 0 - uv_modality = 0 - remaining_discoverable_credentials = 0 Error: Invalid item $ fido2-token -I /dev/hidraw0 proto: 0x02 major: 0x02 minor: 0x00 build: 0x03 caps: 0x05 ( wink, cbor, msg ) version strings: FIDO_2_0, U2F_V2 extension strings: credProtect, hmac-secret aaguid: 42b4fb4a286643b29bf76c6669c2e5d3 options: rk, clientPin fwversion: 0x0 maxmsgsiz: 2200 maxcredcntlst: 0 maxcredlen: 0 maxlargeblob: 0 pin protocols: 1 pin retries: 8 pin change required: false uv retries: undefined Solo 1 Hacker by SoloKeys I bought this key in February 2019 from the official SoloKeys website for $20, plus shipping fee, for a total of $30. This is an old model. The newer one is the Solo 2. The firmware was later upgraded to version `4.1.5 unlocked`. $ solo key version 4 .1.5 unlocked $ ctapcli info Get the Authenticator infomation. Get all data. - versions = [ \"U2F_V2\" , \"FIDO_2_0\" , \"FIDO_2_1_PRE\" ] - extensions = [ \"credProtect\" , \"hmac-secret\" ] - aaguid ( 16 ) = 8876631BD4A0427F57730EC71C9E0279 - options = [( \"rk\" , true ) , ( \"up\" , true ) , ( \"plat\" , false ) , ( \"credMgmt\" , true ) , ( \"clientPin\" , true )] - max_msg_size = 1200 - pin_uv_auth_protocols = [ 1 ] - max_credential_count_in_list = 20 - max_credential_id_length = 128 - transports = [] - algorithms = [] - max_serialized_large_blob_array = 0 - force_pin_change = false - min_pin_length = 0 - firmware_version = 0 - max_cred_blob_length = 0 - max_rpids_for_set_min_pin_length = 0 - preferred_platform_uv_attempts = 0 - uv_modality = 0 - remaining_discoverable_credentials = 0 Error: Invalid item $ fido2-token -I /dev/hidraw0 proto: 0x02 major: 0x00 minor: 0x00 build: 0x00 caps: 0x05 ( wink, cbor, msg ) version strings: U2F_V2, FIDO_2_0, FIDO_2_1_PRE extension strings: credProtect, hmac-secret aaguid: 8876631bd4a0427f57730ec71c9e0279 options: rk, up, noplat, credMgmt, clientPin fwversion: 0x0 maxmsgsiz: 1200 maxcredcntlst: 20 maxcredlen: 128 maxlargeblob: 0 pin protocols: 1 pin retries: 8 pin change required: false uv retries: undefined $ ctapcli cred PIN: Enumerate discoverable credentials. - existing discoverable credentials: 1 /49 - rp: ( id: webauthn.io, name: test ) - credential: ( id: ***, name: test, display_name: test )","tags":"dev","url":"https://tome.one/passkeys-and-security-key-shopping-guide.html"},{"title":"New blog layout","text":"You may have noticed that the blog's visual look changed. I spent some time upgrading its looks and built a new Pelican theme based on Tabler . I love Tabler. I have used it to build a prototype recently. It just looks good by default and it comes with a ton of UI components and an extensive SVG icon library. I also added a new \"Projects\" page to showcase a few open source projects I built, such as the recent Fuzzomatic. I also updated the conferences and talks pages to include my latest talks such as a workshop on Security Keys I gave at Ph0wn with my colleague Sylvain and the Polynonce research we presented this summer at DEF CON in Las Vegas with my colleague Marco. Expect to see a few more blog posts in the coming months. Until then, happy holidays!","tags":"dev","url":"https://tome.one/new-blog-layout.html"},{"title":"2022 research","text":"By the end of this year, I will have given four talks this year. I did some research on GnuPG (GPG) memory forensics with my colleague Sylvain Pelissier and presented that research at Nullcon Berlin 2022 in April, and then at SSTIC2022 in June. I also did some research on a smart home system that was installed in my apartment and presented that at MCH2022, a Dutch hacker camp that was awesome! That was in July. I will present on the same topic at Black Alps 2022 in November.","tags":"dev","url":"https://tome.one/2022-research.html"},{"title":"Virtual talks","text":"I gave two talks , lately. One was about replacing passwords with FIDO2 password authentication at Pass the SALT 2020. I gave the other one with my colleague Tommmaso. It was about an open source Oblivious RAM filesystem prototype I wrote in Rust, called oramfs .","tags":"dev","url":"https://tome.one/virtual-talks.html"},{"title":"BlackAlps19 Blockchain vulnerabilities and exploitation in practice workshop","text":"I gave a 3-hour workshop about blockchain vulnerabilities and exploitation in practice at BlackAlps19 in Yverdon, Switzerland today. Here are the slides: blackalps19-blockchain-workshop.pdf","tags":"dev","url":"https://tome.one/blackalps19-blockchain-vulnerabilities-and-exploitation-in-practice-workshop.html"},{"title":"DEF CON 26 talk","text":"If you were in Las Vegas for DEF CON 26 this year you may have seen a talk I gave with my colleague Yolan Romailler about collecting public keys and breaking them. The video will be uploaded to Youtube eventually and I will update this post with its link. For now, the slides and demo video are available on the Kudelski Security research blog . Edit: Here is the youtube video.","tags":"dev","url":"https://tome.one/def-con-26-talk.html"},{"title":"WireGuard","text":"I co-wrote a blog post on WireGuard with adr13n earlier this month. Read it on research.kudelskisecurity.com .","tags":"dev","url":"https://tome.one/wireguard.html"},{"title":"Playing 10bit HEVC videos on Linux with NVIDIA and mpv","text":"For this to work you will need an NVIDIA GPU. Get the mpv-build scripts, tune the mpv and ffmpeg options and build: #!/bin/sh mkdir -p ~/git cd ~/git git clone https://github.com/mpv-player/mpv-build.git cd mpv-build # write mpv_options file cat << EOF > mpv_options --enable-cuda-hwaccel EOF # write ffmpeg_options file cat << EOF > ffmpeg_options --enable-cuda --enable-cuvid --enable-nonfree EOF # build ./rebuild -j10 Play a 10bit HEVC video with: ~/git/mpv-build/mpv/build/mpv --hwdec=cuda --vo=opengl <VIDEO_FILE> This is especially useful for 60 frame per second 2160p 10bit HEVC videos since most CPUs cannot decode those fast enough for smooth playback.","tags":"dev","url":"https://tome.one/playing-10bit-hevc-videos-on-linux-with-nvidia-and-mpv.html"},{"title":"Setting up gpg-agent","text":"On Arch gpg-agent comes with the gnupg package so no other package is needed if gpg is already installed. I use gpg-agent for temporarily caching the PIN for my Yubikey so that I don't have to type it everytime as well as for ssh-agent emulation. Setup Add the following lines to your ~/.bashrc : # Set GPG TTY export GPG_TTY = $( tty ) # Refresh gpg-agent tty in case user switches into an X session gpg-connect-agent updatestartuptty /bye >/dev/null # Set SSH to use gpg-agent unset SSH_AGENT_PID if [ \" ${ gnupg_SSH_AUTH_SOCK_by :- 0 } \" -ne $$ ] ; then export SSH_AUTH_SOCK = \"/run/user/ $UID /gnupg/S.gpg-agent.ssh\" fi # Start the gpg-agent if not already running if ! pgrep -x -u \" ${ USER } \" gpg-agent >/dev/null 2 > & 1 ; then gpg-connect-agent /bye >/dev/null 2 > & 1 fi Add these lines to ~/.gnupg/gpg-agent.conf : enable-ssh-support default-cache-ttl 10800","tags":"dev","url":"https://tome.one/setting-up-gpg-agent.html"},{"title":"Systemd tips and tricks","text":"Systemd is great. Here are a few things you can do with it. Editing unit files I usually can't remember where unit files are located but that's not an issue. Systemd will let you edit them just by knowing the unit name. $ export SYSTEMD_EDITOR = vim $ sudo -E systemctl edit --full <unit_name> Using the journal like a boss Check out what's going on in the journal live ( tail -f style) for a specific unit: $ journalctl -u <unit_name> -fn200 An incident with nginx happened last night around 21:30? No problem, let's skip directly there: $ journalctl -u nginx --since \"2017-02-18 21:30\" Other examples: $ journalctl -u nginx --since yesterday $ journalctl -u nginx --since today $ journalctl -u nginx --since \"10 hours ago\" --until \"4 hours ago\" $ journalctl -u nginx --since 14 :00 I don't think it's possible to do --since \"yesterday 21:30\" yet. The full date has to be used. I might contribute a patch for that. See man systemd.time for more details. An incident happened and it involves a few units, let's say nginx, mysql and php-fpm? Let's see what happened: $ journalctl -u nginx -u mysql -u php-fpm --since today Quickly store messages in the journal under a SYSLOG_IDENTIFIER: $ echo \"blah blah blah\" | systemd-cat -t my_identifier $ journalctl -t my_identifier Make the journal persistent, assuming you kept the default Storage=auto in /etc/systemd/journald.conf : $ sudo mkdir /var/log/journal Alternatively, Storage=persistent can also be set. Be careful with persistent journal. The journal will now survive reboots but by default its maximum size is set to 10% of the file system it resides in. Additionally the size is capped to 4 GB, even if you use SystemMaxUse=100% so if your unit produces lots of messages you won't be able to keep log history for a long time anyway. Skip directly to the relevant journald.conf manpage section with the following command for more details: $ LESS = '+/SystemMaxUse' man journald.conf Listing enabled unit files $ systemctl list-unit-files --state = enabled Managing logs across all your machines While logs can be forwarded to a master machine using systemd-journal-remote (see manpage for details) you would still be capped to 4GB of logs. It's preferable to send journal messages to a Graylog instance. Use journal2gelf to export systemd messages in the Graylog Extended Log Format (GELF). Graylog has virtual appliances for download, including a docker-compose version which sadly does not allow for proper scaling, i.e. automatically manage volumes for persistence when scaling with Elasticsearch and MongoDB which are used by Graylog. There are however Kubernetes Helm charts for Elasticsearch and MongoDB worth looking into. With Kubernetes 100% managed manual scaling or auto-scaling could be performed.","tags":"dev","url":"https://tome.one/systemd-tips-and-tricks.html"},{"title":"Bash Magic","text":"Over the years I have learned many tips and tricks when working with Bash. Some are well known while others are not. Here are a few tricks that I find the most useful. Searching history So you know you just typed that command but cannot remember what it was. Fine, just type <ctrl>-R . Your prompt will then look something like this: (reverse-i-search)`': Start typing part of your command and bash will find it for you. Search results will be narrowed down as you keep typing. Cycle forward through search results by hitting <ctrl>-R multiple times. Cycling backwards through history with <ctrl>-S requires enabling XON/XOFF flow control with the following command: stty -ixon Otherwise your terminal will freeze upon hitting <ctrl>-S . Use <ctrl>-Q to unfreeze. Press Enter to execute the search result and use the arrows to edit the command before executing. Customizing the prompt The prompt is defined by the PS1 variable. The default prompt is quite succinct: PS1='\\s-\\v\\$' The result looks like this: bash - 4.3 $ It can be improved by displaying useful information such as the date, hostname, username, current working directory and much more. Here is the PS1 I've been using these days: PS1='‚îå‚îÄ[\\e[0;32m\\t\\e[0m]‚îÄ[\\e[0;36m\\u\\e[0m\\e[0m \\e[0;33m\\w\\e[0m]\\n‚îî‚îÄ>\\$ ' It looks like that: ‚îå‚îÄ [ 13 : 33 : 37 ] ‚îÄ [ john ~ / git / foo / bar / www ] ‚îî‚îÄ > $ See the Bash manual for more details about Bash variables. Moving and Erasing Hit <ctrl>-A to move the caret to the beginning of the line. <ctrl>-E will move to the end of the line. Use <alt>-F to move one word forward and <alt>-B to move one word backwards. <ctrl>-U will delete everything to the left of the caret while <ctrl>-K will delete everything under and to the right of the caret. Type <ctrl>-W to delete the word to the left (until the first space character). Use <alt>-Backspace to delete the word to the left until the next special character. This is useful when working with paths and you want to delete until the next / to the left. Recalling history Recall the last argument of the last command with <alt>-_ (<alt>-underscore). Run the previous command with sudo in front: sudo !! Expansion Create a backup copy of a file without typing the file path twice: cp /path/to/file{,.bak} Configuring history Increase the number of commands stored in the history and save the time and date each command was executed on: HISTSIZE=100000 HISTFILESIZE=20000 HISTTIMEFORMAT=\"[%F %T %Z (%z)] \" The history is stored in ~/.bash_history and can be displayed with the history command. Copy and paste Paste the contents of the clipboard with <shift>-Insert . Run a command with the last argument from your clipboard: # define a function xc() xc() { $@ \"$(xclip -o)\" } # use with xc my_command # will execute: my_command <clipboard_contents> How is this more useful than pasting the clipboard contents with <shift>-Insert you say? This will save you enclosing the clipboard contents with quotes since the clipboard contents are passed as a single argument to my_command . Useful aliases Package management: alias i='sudo -E pacman -S' alias r='sudo -E pacman -Rs' alias s='pacman -Ss' alias syu='sudo -E pacman -Syu' Listing files and grepping: alias ls='ls --color=auto --group-directories-first -lh' alias lsgrep='ls | grep -i' Systemd: alias sc='sudo -E systemctl' Systemd will be the topic of another post. Enjoy!","tags":"dev","url":"https://tome.one/bash-magic.html"},{"title":"Spark Summit Europe 2016","text":"I attended Spark Summit Europe 2016 in Brussels this year in October, a conference where Apache Spark enthusiasts meet up. I've been using Spark for nearly a year now on multiple projects and was delighted to see so many Spark users at Square Brussels. There were three trainings to choose from on the first day. I went for \"Exploring Wikipedia with Spark (Tackling a unified case)\". The class was taught in Scala and Databricks notebooks were used. Databricks is a cloud platform that lets data scientists use Spark without having to setup or manage a cluster themselves. Databricks uses AWS as their backend. Clusters can be started and then attached to notebooks where code can be executed on the attached cluster. The class started with a recap of the basics, covering multiple APIs, including RDDs, Dataframes and the new Datasets. We used publicly available Wikipedia datasets and leveraged Spark SQL, Spark Streaming, GraphFrames, UDFs and machine learning algorithms. I was impressed to see how easy it was to run code snippets on the Databricks platform and get insights into the data. Another great feature is the support for mixing languages in a notebook. For instance a UDF can be defined and registered in Python and can then be used in Scala. The other two trainings which I wasn't able to attend were \"Apache Spark Essentials (Python)\" and \"Data Science with Apache Spark\". The talks The following days were conference days. Usually each day started with keynotes and then there were three or four talks to choose from every 30 minutes. I will highlight some of the talks and keynotes I attended. Simplifying Big Data Applications with Apache Spark 2.0 Spark 2.0 was released and brings many improvements over the 1.6 branch, namely: Performance improvements with whole-stage code generation and vectorization Unified API: Dataframes are now just an alias for Datasets The new SparkSession single entry point. This replaces SparkContext, StreamingContext, SQLContext, etc. The Next AMPLab: Real-Time, Intelligent, and Secure Computing Spark was born at AMPLab . We were shown what projects AMPLab is currently working on and thus what can be expected in the next 5 years for Spark. They currently have two main projects: Drizzle and Opaque. Drizzle aims at reducing latency in Spark Streaming while Opaque is an attempt at improving security in Spark, for instance by protecting against pattern recognition attacks. Spark's Performance: The Past, Present, and Future Performance in Spark 2.0 is improved with whole-stage code generation, a new technique which will optimize the code of the whole pipeline and can boost performance by one order of magnitude in some cases. Another technique used to improve performance is vectorization, or in other words, using an in-memory columnar format for faster data access. Databricks published a blog post discussing this. How to Connect Spark to Your Own Datasource The author of the MongoDB Spark connector shared his experience in writing a Spark connector. There is a lack of official documentation on writing these so the best way to start writing your own connector is to look at how others did it, for example the Spark Cassandra connector . Dynamic Resource Allocation, Do More With Your Cluster This technique is useful for shared clusters and jobs of varying load. In this talk we were shown some parameters that can be set for optimizing dynamic resource allocation on a Spark cluster. Vegas, the Missing MatPlotLib for Spark Two engineers from Netflix showed their project called Vegas . This project will generate HTML code that can be used on web pages. Vegas also supports Apache Zeppelin notebooks, has console support and can render to SVG. Vegas uses Vega-Lite underneath. It is currently in beta stage. SparkLint: a Tool for Monitoring, Identifying and Tuning Inefficient Spark Jobs Across Your Cluster Groupon announced the availability of SparkLint , a performance debugger for Spark. It can detect over-allocation and has CPU utilization graphs for Spark jobs. SparkLint is available on Github. Spark and Object Stores ‚ÄîWhat You Need to Know This talk gives a set of optimal parameters to use when working with Object Stores and Spark. When using the Amazon S3 API, make sure to use the new s3a:// protocol in your URLs. This is the only one that is currently supported. Mastering Spark Unit Testing A few tips and tricks from Blizzard were presented for unit testing Spark jobs. The main ideas were that one should not use a Spark context if it's not necessary. Code can usually be tested outside of a Spark job. If it's really necessary to run a Spark job in your test, then use the local master and run it on your local machine. You can then set breakpoints for instance in IntelliJ Idea and debug both driver and executor code. A cool idea that the speaker gave was to share the Spark context across various unit tests so that the initialization is done only once and the tests are running faster. Apache Spark 2.0 Performance Improvements Investigated With Flame Graphs Flame Graphs are a great visualization tool that can be used to profile Spark jobs in order to find the most frequent code paths and optimize bottlenecks. This talk is about the use of Flame graphs at CERN in order to analyze the performance of Spark 1.6 and 2.0. TensorFrames: Deep Learning with TensorFlow on Apache Spark Databricks presented TensorFrames , a bridge between Spark and TensorFlow . A TensorFlow graph can be defined and used as a mapper function that can be applied to a Dataframe. TensorFrames can bring a huge performance increase when running on GPUs. Apache Spark at Scale: A 60 TB+ Production Use Case Facebook uses Spark at scale and during this talk they presented a few tips and tricks that they found while working with Spark. They use Flame Graphs for profiling. They highlighted that the thread dump function available in the Spark UI is useful for debugging. They gave interesting ideas for configuration: Use memory off heap Use parallel GC instead of G1GC Tune the shuffle service (number of threads, etc.) Configure the various buffer sizes They published a blog post about this. Apache Kudu and Spark SQL for Fast Analytics on Fast Data An engineer from Cloudera presented Apache Kudu , a top level Apache project that sits between HDFS and HBase . The speaker revealed an interesting fact during the Q&A session: Kudu does not store its data on HDFS, but rather on a local file system. Kudu is a data store that has some of the advantages of the Parquet file format: it's a columnar store. Support for Kerberos in Kudu is coming soon. SparkOscope: Enabling Apache Spark Optimization Through Cross-Stack Monitoring and Visualization SparkOscope is an IBM research project. It collects OS-level metrics while Spark jobs are running. It does not guarantee that the metrics correspond to the resource usage of the Spark job. In the event that other processes are running at the same time as the Spark job that is being observed then the metrics will include usage of multiple processes unrelated to Spark jobs. SparkOscope is available on Github. Problem Solving Recipes Learned from Supporting Spark OutOfMemoryErrors usually happen when allocating too many objects. Tune the spark.memory.fraction setting and do not allocate objects in tight loops. Be careful when allocating objects in mapPartitions() for instance. NoSuchMethodError is usually thrown when there is a library version mismatch. Try to upgrade or downgrade Spark, change the library loading order or shade libraries to fix this. Use spark.speculation to restart slow-running tasks. Use df.explain to debug queries on dataframes. Containerized Spark on Kubernetes There was this excellent talk by William Benton from Red Hat about running Spark on Kubernetes. Don't miss out on this one! Spark SQL 2.0 Experiences Using TPC-DS Very interesting talk and Q&A session about running a large scale benchmark with Spark SQL on a $5.5 million cluster. About 90% of the 99 queries defined in the TPC-DS specification were runnable on Spark SQL. See the related blog post . The talks I missed There are a few more talks that I couldn't attend but that I will watch as soon as the video streams become available: No One Puts Spark in the Container Hive to Spark‚ÄîJourney and Lessons Learned Adopting Dataframes and Parquet in an Already Existing Warehouse A Deep Dive into the Catalyst Optimizer Closing words A few interesting things/trends I heard at Spark Summit: Parquet is an efficient, fast, columnar file format Many people use Databricks notebooks. You don't have to manage your own cluster. There is no better API (RDD, Dataframes, Datasets), it's a question of preference Dataframes do not replace RDDs but they have an advantage one should be aware of: the Catalyst optimizer will rewrite your poorly optimized queries when using Spark SQL and dataframes. This is not true when using the low-level RDD API directly. Presentation slides and recordings from the event will be available on the Spark Summit website by November 4. Update: cross-posted on research.kudelskisecurity.com","tags":"dev","url":"https://tome.one/spark-summit-europe-2016.html"}]}